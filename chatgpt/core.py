"""Classes and functions used by different components of the package.
"""

from typing import Any, Dict, List, Optional
from uuid import UUID
from langchain import OpenAI
from langchain.agents import AgentType, Tool, initialize_agent
from langchain.callbacks.base import AsyncCallbackHandler
from langchain.chains import ConversationChain
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import LLMResult

from chatgpt import OPENAI_API_KEY
from chatgpt.langchain import memory as mem
from chatgpt.langchain import parsers, prompts
from chatgpt.types import FinishReason


class GenerationResults():
    """Encapsulates the results of a text generation operation."""

    text: str
    """The generated text."""
    finish_reason: FinishReason
    """The reason why the generation process finished."""
    prompt_tokens: int
    """The number of tokens used in the prompt."""
    generated_tokens: int
    """The number of tokens generated by the model."""

    def __init__(self, result: LLMResult):
        """Initializes a new instance of the `GenerationResults` class.

        Args:
            result (LLMResult): The result of a text generation operation.
        """
        # parse finish reason
        finish_reason = FinishReason.UNDEFINED
        if info := result.generations[0][0].generation_info:
            finish_reason = FinishReason(
                info.get("finish_reason") or FinishReason.UNDEFINED
            )
        # parse token usage
        prompt_tokens, generated_tokens = 0, 0
        if (out := result.llm_output) and (usage := out.get("token_usage")):
            prompt_tokens = usage.get("prompt_tokens")
            generated_tokens = usage.get("completion_tokens")
        # set attributes
        self.text = result.generations[0][0].text
        self.finish_reason = finish_reason
        self.prompt_tokens = prompt_tokens
        self.generated_tokens = generated_tokens


class StreamHandler:
    def __init__(self) -> None:
        self.handler = _StreamHandler(self)

    def on_start(self) -> None:
        pass

    def on_new_token(self, token: str) -> None:
        pass

    def on_end(self, results: GenerationResults) -> None:
        pass



class _StreamHandler(AsyncCallbackHandler):
    """Handles the generation of tokens by a model. The Handler is called when
    a new token is generated or when the model finishes generating text."""

    def __init__(self, callback: StreamHandler):
        """Initializes a new instance with a token handler.

        Args:
            handler (callable): A function that handles generated tokens.
        """
        super().__init__()
        self._handlers = handlers

    async def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], *, run_id: UUID, parent_run_id: UUID | None = None, **kwargs: Any) -> Coroutine[Any, Any, None]:
        return await super().on_llm_start(serialized, prompts, run_id=run_id, parent_run_id=parent_run_id, **kwargs)

    async def on_llm_new_token(self, token: str, **kwargs):
        """Called when a new token is generated.

        This method is called when a new token is generated by the model. It passes the generated token to the `gen` function provided in the constructor.

        Args:
            token (str): The generated token.
            **kwargs: Arbitrary keyword arguments.

        Returns:
            None
        """
        await self._handle_token(token)

    async def on_llm_end(self, response: LLMResult, **kwargs):
        """Called when the model finishes generating text.

        This method is called when the model finishes generating text. It creates a `GenerationResults` object from the response and passes the generated text to the `gen` function provided in the constructor.

        Args:
            response (LLMResult): The result of the text generation operation.
            **kwargs: Arbitrary keyword arguments.

        Returns:
            None
        """
        results = GenerationResults(response)
        await self._handle_token(results)

    async def on_

    async def on_llm(self, *args, **kwargs):
        """Expected by callback managers but not used."""
        pass
